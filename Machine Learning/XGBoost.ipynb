{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost \n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "\n",
    "* Not a specific machine learning algorithm\n",
    "* Concept that can be applied to a set of machine learning models\n",
    "\"Meta-algorithm\"\n",
    "* Ensemble meta-algorithm used to convert many weak learners into a strong learner\n",
    "\n",
    "**weak Learner** - Learners which are slightly better than randomness eg. Decision tree with accuracy greater than 50%\n",
    "\n",
    "**How boosting works?**\n",
    "\n",
    "* Iteratively learning a set of weak models on subsets of the data \n",
    "* Weighing each weak prediction according to each weak learner's performance\n",
    "* Combine the weighted predictions to obtain a single weighted prediction\n",
    "\n",
    "... that is much better than the individual predictions themselves!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What's the difference from Random Forrest?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how to install \n",
    "#pip install xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   1.0\n"
     ]
    }
   ],
   "source": [
    "# load the dataset \n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X,y = iris.data , iris.target\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:  \", (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to NOT use XGBoost?**\n",
    "\n",
    "* Image recognition\n",
    "* Computer vision\n",
    "* Natural language processing and understanding problems\n",
    "* When the number of training samples is significantly smaller than the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Functions and Base Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective Functions**\n",
    "\n",
    "* Quantifies how far off a prediction is from the actual result\n",
    "* Measures the difference between estimated and true values for some collection of data\n",
    "* Goal: Find the model that yields the minimum value of the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Loss Functions and XGBoost\n",
    "Loss function names in xgboost:\n",
    "* **reg:linear** - use for regression problems\n",
    "* **reg:logistic** - use when you want probability rather than just decision\n",
    "* **binary:logistic** - use for classification problems when you want just decision, not probability\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Learners and Why We Need Them?**\n",
    "\n",
    "* XGBoost involves creating a meta-model that is composed of many individual models that combine to give a final prediction\n",
    "* Individual models = base learners\n",
    "* Want base learners that when combined create final prediction that is non-linear\n",
    "* Each base learner should be good at distinguishing or predicting different parts of the dataset\n",
    "* Two kinds of base learners: **tree** and **linear**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trees as Base Learners in Scikit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 173308.244360\n"
     ]
    }
   ],
   "source": [
    "boston_data = pd.read_csv(\"../datasets/boston_housing.csv\")\n",
    "X, y = boston_data.iloc[:,:-1],boston_data.iloc[:,-1] \n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,\n",
    "test_size=0.2, random_state=123)\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear',\n",
    "n_estimators=10, seed=123)\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Linear Base Learner in XGBoost API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 98285.815883\n"
     ]
    }
   ],
   "source": [
    "boston_data = pd.read_csv(\"../datasets/boston_housing.csv\")\n",
    "X, y = boston_data.iloc[:,:-1],boston_data.iloc[:,-1] \n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y,\n",
    "test_size=0.2, random_state=123)\n",
    "DM_train = xgb.DMatrix(data=X_train,label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test,label=y_test)\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train,\n",
    "num_boost_round=10)\n",
    "preds = xg_reg.predict(DM_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Model\n",
    "\n",
    "Why tune? because it gives better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Untuned Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Tuned rmse: 33288.916504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "housing_data = pd.read_csv(\"../datasets/ames_housing_trimmed_processed.csv\")\n",
    "\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "untuned_params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=untuned_params, nfold=4, num_boost_round=200, metrics=\"rmse\",as_pandas=True, seed=123)\n",
    "\n",
    "print(type(tuned_cv_results_rmse))\n",
    "\n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned rmse: 31003.937989\n"
     ]
    }
   ],
   "source": [
    "housing_data = pd.read_csv(\"../datasets/ames_housing_trimmed_processed.csv\")\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]], housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "tuned_params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3, 'learning_rate': 0.1, 'max_depth': 5}\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=tuned_params, nfold=4, num_boost_round=200, metrics=\"rmse\",as_pandas=True, seed=123)\n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common tree tunable parameters\n",
    "\n",
    "* **learning rate:** learning rate/eta\n",
    "* **gamma:** min loss reduction to create new tree split\n",
    "* **lambda:** L2 reg on leaf weights\n",
    "* **alpha:** L1 reg on leaf weights\n",
    "* **max_depth:** max depth per tree\n",
    "* **subsample:** % samples used per tree \n",
    "* **colsample_bytree:** % features used per tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Load the kidney disease dataset \n",
    "2. Try to predict the disease given blood values \n",
    "\n",
    "**Main Challenges : Many missing values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References :\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/model.html\n",
    "https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
